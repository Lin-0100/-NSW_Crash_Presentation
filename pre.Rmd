---
title: "NSW Road Crash Severity Prediction"
author: "Group W05_G01 | STAT5003 | The University of Sydney"
date: "2025-10-25"
output:
  xaringan::moon_reader:
    css: ["default", "metropolis", "custom-style.css"]
    self_contained: true
---
## Our Research Question

### 🚧 Why it matters
Road safety is a pressing issue in New South Wales, especially in high-traffic areas like Greater Sydney.<br><br>
Thousands of crashes occur every year,many resulting in serious injuries or fatalities. <br><br>
Understanding the key contributing factors is crucial for prevention and policy design.

### ❓ Can we predict crash severity?
Can we build a predictive model using time, location, road, weather, and vehicle data to classify NSW crashes into different severity levels?

### 🎯 Why it’s useful
- Identify high-risk zones. <br><br>
- Improve emergency response.   <br><br>
- Support NSW's <strong>Vision Zero</strong> strategy to eliminate serious road trauma.

---
## From Raw Data to Rich Features

### 📦 Data Source  
We used official crash records from the [NSW Centre for Road Safety](https://data.nsw.gov.au/data/dataset/2-nsw-crash-data).   <br><br>
The dataset includes crashes across New South Wales from 2019 to 2023.

### 🔗 Dataset Merging  
To enrich the features, we merged two datasets:  <br><br>
– `nsw_crash_data` (incident-level crash records)  <br><br>
– `nsw_vehicle_data` (details about vehicle types, usage, and involvement)<br><br>
This allows us to link each crash to detailed vehicle information.

### 🧠 Why merge the data?  
By combining crash and vehicle data, we gain access to more predictive features:  <br><br>
– Vehicle type (car, motorcycle, truck...)  <br><br>
– Number of vehicles involved  <br><br>
These features help us better understand what factors are associated with crash severity.

---
##  Data Preprocessing

###1. Data Cleaning & Selection:
- Removed irrelevant (e.g., route, coordinates) and leakage-related variables (e.g., number killed)<br><br>
- Dropped redundant or high-missing (>90%) features

###2. Transformation & Encoding:
- Most categorical variables were mapped to integers&lt<br><br>
- Street type used label encoding (better for tree models)<br><br>
- Identifying feature type merged rare categories (40 → 16)

###3. Feature Engineering:
- Distance was right-skewed — handled by exponential binning to reduce noise<br><br>
- RUM & DCA codes were compressed to capture key road-type information

###4. Missing Value Imputation:
- Applied mode or zero filling depending on feature type&lt;<br><br>

### ✅ Result:
Cleaned dataset with 93,491 records and 36 features, ready for modeling.

---
## Exploring the Dataset Structure
###1. Target Variable Overview
   • Distribution of crash severity (5 classes)<br><br>
   • Class imbalance → evaluation challenge
<div style="display: left; justify-content: left; align-items: left;">
  <img src="images/i_8.png" width="20%">
</div>

###2. Data Quality Check
<div style="display: left; justify-content: left; align-items: left;">
  <img src="images/i_9.png" width="30%">
</div>
   • Missing values handled by mode imputation<br><br>
   • Boxplots reveal few outliers (distance, vehicle count)

###3. Frequency Patterns
<div style="display: left; justify-content: left; align-items: left;">
  <img src="images/i_13.png" width="45%">
</div>
   • Most crashes: weekdays, daylight, afternoon/evening, Fine, moderate speed, urban, straight roads

---
## Feature Insights
### 1.Correlation Analysis
The correlation analysis was conducted across **numerical**, **categorical**, **boolean**, **contextual** to evaluate their association with crash severity.
<div style="display: flex; justify-content: left; align-items: left;">
  <img src="images/i_16.png" width="17%">
  <img src="images/i_10.png" width="25%">
  <img src="images/i_15.png" width="36%">
</div>

### 2.Feature-Outcome Relationship
<div style="display: flex; justify-content: left; align-items: left;">
  <img src="images/i_17.png" width="55%">
  <img src="images/i_18.png" width="48%">
</div>

### 3.Dimension Reduction
 • Class clusters overlap in 2D → no clear boundary  <br><br>
 • Need for **non-linear models** (RF, Boosting) to capture interactions
<div style="display: left; justify-content: left; align-items: left;">
  <img src="images/i_14.png" width="45%">
</div>

---
## Modelling Plan

### 1.Modelling Strategy
- Naive Bayes (Baseline, Probabilistic) <br><br>
- KNN (Instance-based) <br><br>
- Decision Tree (Interpretable Rules) <br><br>
- Random Forest (Bagging, Robustness) <br><br> 
- Boosting (XGBoost/GBM, Frontier)

### 2.Evaluation Strategy
- Primary: Macro-F1 (balances rare vs common classes).<br><br>
- Supporting: Balanced Accuracy, per-class P/R, ROC-AUC.<br><br>
- Focus on error patterns in middle classes (Moderate/Minor) for discussion.

---
## Naive Bayes

### 1. Model Development

**Data Preparation:**  
- Missing values → filled with mode <br><br>
- Variables → converted to factors<br><br>
- Used stratified sampling (70/30) to maintain class balance 


**Tuning:**  
- 5-fold cross-validation tested Laplace (0–2) and usekernel (TRUE/FALSE).  <br><br>
- Best = (Laplace = 0, usekernel = TRUE), but improvement small  <br><br>
- kept Laplace = 1 baseline for clarity and consistency


### 2. Model Performance Overview
- Accuracy = 38.8% Kappa = 0.219<br><br>
- F1 (by class) = 0.55 (No Injury), 0.28 (Moderate), 0.11 (Fatal) <br><br>
- Macro AUC = 0.73 <br><br>
<div style="display: flex; justify-content: left; align-items: left;">
  <img src="images/N1.png" width="35%">
  <img src="images/N2.png" width="35%">
  <img src="images/N3.png" width="33%">
</div>
- Moderate accuracy overall, strongest on non-injury, weakest on fatal cases
---
## Naive Bayes

### 3.Insight  
- **Poor lighting** and **wet surface conditions** → higher severity risk.  <br><br>
- Naïve Bayes captured **reasonable relationships** between environment and injury level.  <br><br>
- Model is **simple** and **explainable — useful for interpreting patterns in crash data.  

### 4.Reflection & Future Work  

**Limitations**
- Mid-level classes → feature overlap → harder to separate  <br><br>
- Naïve Bayes assumes feature independence  <br><br>
- Class imbalance → bias toward common (non-injury) cases  

**Future Directions**
- Apply SMOTE / upsampling to balance classes  <br><br>
- Try ensemble methods (e.g., Random Forest, XGBoost)  <br><br>
- Add contextual features (e.g., road type, time of day)
---
## KNN

### 1. Model Development
#### 🎯 Why KNN
  I used K-Nearest Neighbors because it’s simple, non-parametric, and good at capturing local patterns without assuming a fixed model form.

#### 🧩 Data Preparation
Cleaned and standardized all variables before modeling.
  - Removed missing and inconsistent records.
  - Detected numeric outliers but kept valid extremes since they reflect real crashes.
  - Renamed and factorized the target variable, dropped IDs, and scaled all numeric predictors.
  - Applied PCA (95% variance) to remove noise and multicollinearity.

✅ The data was fully ready for modeling after preprocessing

#### 📊 Model Accuracy and Optimization
##### ===== Macro-Averaged Metrics (Final) =====
  - Accuracy: 42.53 %
  - Macro Precision: 0.398
  - Macro Recall: 0.335
  - Macro F1: 0.338
  - KNN model reached about 40% accuracy, which is reasonable for this imbalanced dataset.
##### Improved performance by:
  - Using 5-fold cross-validation to reduce random bias.
  - Tuning k (neighbors) for best validation accuracy and avoiding overfitting.
  - Applying PCA and scaling to refine distance metrics.
---
## KNN
🟦 Figure A – Crash Severity Distribution
  - This plot shows the class distribution of CrashSeverity, revealing a strong imbalance where minor crashes dominate.
  - Such skewness can bias KNN predictions toward the majority class, reducing accuracy if not addressed

🟦 Figure B – Outlier Distribution (Numeric Variables)
  - Boxplots of the ten most variable numeric features reveal several valid outliers representing rare crash conditions.
  - Their presence underscores the importance of scaling features to ensure fair distance comparisons in KNN.

🟩 Figure C – PCA Variance Explanation (Retain 95%)
  - The cumulative variance plot shows that the first r n_comp components retain 95% of total variance.
  - PCA reduces dimensionality and multicollinearity, improving KNN stability against high-dimensional effects.

🟥 Figure D – 5-Fold Cross Validation Accuracy (KNN)
  - Cross-validation results show stable performance across folds (≈40–43% accuracy).
  - The median-selected k = r final_k balances bias and variance under class imbalance.
  <div style="display: left; justify-content: left; align-items: left;">
  <img src="images/K1.png" width="80%"></div>
---
## KNN
🟦 Figure E – Confusion Matrix (Hold-Out Set, k = r final_k)
  - The heatmap shows KNN’s classification performance on test data.
  - Diagonal cells denote correct predictions; off-diagonal ones reveal confusion mainly between moderate and minor crashes.
  - Fatal and non-casualty classes remain more distinct, indicating stronger performance at the extremes.

🟨 Figure F – Per-Class Precision, Recall, and F1
  - This grouped bar chart summarizes precision, recall, and F1 for each crash-severity level.
  - Rare classes (e.g., fatal) show lower recall, while frequent ones achieve higher precision.
  
### Conclusion
  - The KNN model achieved moderate predictive performance, performing best on extreme crash severities but struggling with middle categories due to class imbalance and feature overlap.
  - Applying resampling, feature weighting, or advanced models could further enhance classification fairness and accuracy.
<div style="display: left; justify-content: left; align-items: left;">
  <img src="images/K2.png" width="70%"></div>
---
## Decision Tree
---
## Decision Tree
---
## Random Forest

### 1. Random Forest Overview

- **Ensemble method** → combines multiple decision trees<br><br>
- **Bagging** (Bootstrap Aggregation) → reduces overfitting<br><br>
- Captures **non-linear** patterns in complex datasets<br><br>
- **MDS** visualization confirmed non-linear feature relationships<br><br>
- **Robust & reliable** for crash severity classification with diverse factors

### 2. Model Development

- Target variable **highly imbalanced** (Fatal very rare) → Applied **upsampling** → balanced training data<br><br>
-	Trained RF: ntree=500, mtry≈5, importance=TRUE<br><br>
<div style="display: flex; justify-content: left; align-items: left;">
  <img src="images/i_2.png" width="35%" style="margin-right: 2%;">
  <img src="images/i_3.png" width="35%">
</div>
- **OOB error** = 24.55% → **stable overall**；Class errors → C3: 52.1%, C5: 33.4%, C2/C4: ~18%, C1: 0%<br><br>
- Strong on extremes (Fatal, Non-casualty), weak on mids (Moderate)<br><br>
-	**Insight** → mid-severity overlap remains<br><br>
- **Next** → richer features / cost-sensitive / tuning (mtry, nodesize)

---
## Random Forest

### 3. Model Evaluation

<div style="display: flex; justify-content: left; align-items: left; margin-top: 15px；">
  <img src="images/i_4.png" width="35%">
  <img src="images/i_5.png" width="35%">
</div>

### 4. 3-fold Cross Validation
<div style="display: flex; justify-content: center; align-items: center; text-align: center;">
  <img src="images/i_6.png" width="40%">
</div>

- Mid–severity (Moderate, Minor) → **still overlapping**<br><br>
- Confirms **robust & generalizable** performance<br><br>
- Test results are **stable**, **reliable**

---
## Random Forest

### 5. Variable Importance Analysis
<div style="display: left; justify-content: left; align-items: left;">
  <img src="images/i_7.png" width="50%">
</div>
- Strong influence from **contextual & environmental** features<br><br>
- Indicates model captures **real-world** crash patterns

### 6.Reflection & Future Work

- Model achieved **moderate** but **stable** performance (Balanced Accuracy ≈ 0.63, Macro AUC ≈ 0.76)<br><br>
- Strength: handled **class imbalance** via oversampling<br><br>
- Limitation: confusion between **mid-severity** levels<br><br>
- Next steps:<br><br>
  a. Explore **XGBoost** for finer class boundaries<br><br>
	b. Add **richer contextual** features (driver, vehicle, response time)
	
---
## Boosting

### 1. Model development
- Grid search(with 5-fold cross-validation):<br><br>
- nrounds, max_depth, lr, gamma, <br><br>
- colsample_bytree(Fraction of features randomly sampled per tree)<br><br>
- min_child_weight(Minimum sum of instance weight in a child)<br><br>
- subsample(Fraction of samples randomly sampled per tree)<br><br>

- best parameter combination:nrounds=200, max_depth=7, eta=0.05, gamma=1, colsample_bytree=0.7, min_child_weight=3, subsample=0.7

---
## Boosting

### 2. Model performance
<div id="boosting_plots" style="display: flex; justify-content: left; align-items: flex-start; gap: 10px;">
  <img src="images/B1.png" width="30%">
  <img src="images/B2.png" width="30%">
  <img src="images/B3.png" width="30%">
</div>

<div style="font-size:18px; line-height:1.6;">
<p><strong>Moderate Overall Accuracy:</strong> The model achieves ~54.6% accuracy, better than random but far from high performance.</p>

<p><strong>High-Frequency Bias:</strong> Class X5 dominates predictions (high recall, moderate precision), indicating the model favors the majority class.</p>

<p><strong>Low-Frequency Underfitting:</strong> Minority classes (X1, X4) have extremely low recall, meaning most of their samples are misclassified.</p>

<p><strong>Imbalanced Class Performance:</strong> Macro-F1 (0.439) is much lower than Weighted-F1 (0.529), reflecting uneven performance across classes.</p>

<p><strong>Partial Strengths:</strong> Some precision is acceptable for low-frequency classes, and majority class predictions are reliable, but overall balance is poor.</p>
</div>
---
## Model Performed Compared
---
## Final Insights & Future Work










